{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as lr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 27, 246, 239, 177], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "def quantization_u_law(x):\n",
    "    \"\"\"\n",
    "    quantizing 16 bit audio inputs to 0-255 range using meu law quantization\n",
    "    assuming the audio input is already normalized in range [-1, 1]\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    u = 255\n",
    "    mu_law_output = np.sign(x) * (np.log(1 + u*np.abs(x)))/(np.log(1+u))\n",
    "    quantized = ((mu_law_output + 1)/2 * 255).astype(np.uint8)\n",
    "    return torch.tensor(quantized)\n",
    "\n",
    "print(quantization_u_law([-0.3, 0.7, 0.5, 0.03]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(out_file):\n",
    "    processed_files = []\n",
    "    for file in os.listdir(r\"C:\\Users\\tanbi\\door\\hall_projects\\audio_replicate\\models\\waveNET\\Datasets\"):\n",
    "        audio_file = os.path.join(r\"C:\\Users\\tanbi\\door\\hall_projects\\audio_replicate\\models\\waveNET\\Datasets\", file)\n",
    "        if os.path.isfile(audio_file):\n",
    "            audio_data, _ = lr.load(audio_file, sr=16000, mono=True)\n",
    "            # process data (skipping right now)\n",
    "            processed_files.append(audio_data)\n",
    "    np.savez(out_file, *processed_files)\n",
    "# createDataset(r\"C:\\Users\\tanbi\\door\\hall_projects\\audio_replicate\\models\\waveNET\\data\\processed_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_0\n",
      "[ 1.1830350e-05 -8.9085406e-06  2.2375611e-06 ...  3.8030726e-04\n",
      " -2.5518136e-03 -7.0407242e-03]\n",
      "arr_1\n",
      "[-0.00463775  0.0014874  -0.00155717 ...  0.00877299  0.00386699\n",
      "  0.00637478]\n",
      "arr_2\n",
      "[1.0861059e-02 1.3355248e-02 2.1995592e-03 ... 5.5337507e-05 7.6695425e-05\n",
      " 3.9345337e-05]\n"
     ]
    }
   ],
   "source": [
    "# TEST block\n",
    "\"\"\"\n",
    "from numpy import load\n",
    "\n",
    "data = load('../data/proceed_data.npz')\n",
    "lst = data.files\n",
    "for item in lst:\n",
    "    print(item)\n",
    "    print(data[item])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19, dtype=torch.uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST BLOCK\n",
    "\"\"\"\n",
    "audio_data, _ = lr.load(\"../Datasets/Speaker27_000.wav\", sr=16000, mono=True)\n",
    "quantized_audio_data = quantization_u_law(audio_data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 dataset_path,\n",
    "                 out_path, \n",
    "                 input_length, \n",
    "                 target_length=1, \n",
    "                 sampling_rate=16000, \n",
    "                 mono=True):\n",
    "        \n",
    "        self.dataset_path = dataset_path\n",
    "        self.out_path = out_path\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.input_length = input_length\n",
    "        self.target_length = target_length\n",
    "        self.mono = mono\n",
    "        self.sample_loc_util = [0]\n",
    "        \n",
    "        if not os.path.isdir(dataset_path):\n",
    "            raise FileNotFoundError(f\"dataset path {dataset_path} not found!!\")\n",
    "        self.createDataset(self.out_path)\n",
    "\n",
    "        # out_file = f\"{self.out_path}.npz\" if not self.out_path.endswith(\".npz\") else self.out_path\n",
    "        # if not os.path.exists(out_file):\n",
    "        #     raise FileNotFoundError(f\"Processed dataset file not found: {out_file}\")\n",
    "        self.data = np.load(f\"{self.out_path}.npz\", mmap_mode='r')\n",
    "\n",
    "\n",
    "    def createDataset(self, out_file):\n",
    "        processed_files = []\n",
    "        for file in os.listdir(self.dataset_path):\n",
    "            audio_file = os.path.join(self.dataset_path, file)\n",
    "            if os.path.isfile(audio_file):\n",
    "                audio_data, _ = lr.load(audio_file, sr=self.sampling_rate, mono=self.mono)\n",
    "                audio_data = quantization_u_law(audio_data)\n",
    "                # process data (skipping right now)\n",
    "                # --------------------------------#\n",
    "                #                                 #\n",
    "                #              TODO               # \n",
    "                #              TODO               #\n",
    "                #                                 #\n",
    "                # --------------------------------#\n",
    "                processed_files.append(audio_data)\n",
    "                self.sample_loc_util.append(self.sample_loc_util[-1] + (len(audio_data)-self.input_length))\n",
    "        np.savez(out_file, *processed_files)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.sample_loc_util[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            args: \n",
    "            --- index (idx)\n",
    "\n",
    "            --- locates the audio segment the index lies in and then the local index that is the index in that audio segment from where a sample \n",
    "            --- of inputs and targets will be retrieved \n",
    "        \"\"\"\n",
    "        upper_bound=0\n",
    "        data_idx = 0\n",
    "        for i in range(1, len(self.sample_loc_util)):\n",
    "            if idx < self.sample_loc_util[i]:\n",
    "                data_idx=i-1\n",
    "                break\n",
    "        local_idx = idx - self.sample_loc_util[data_idx]\n",
    "        \n",
    "        audio_segment = self.data[f\"arr_{data_idx}\"]\n",
    "\n",
    "        input_seq = audio_segment[local_idx : local_idx + self.input_length]\n",
    "        target_seq = audio_segment[local_idx+self.input_length : local_idx+self.input_length + self.target_length]\n",
    "        \n",
    "        return torch.tensor(input_seq, dtype=torch.float32), torch.tensor(target_seq, dtype=torch.float32)\n",
    "    \n",
    "    def quantization_u_law(x):\n",
    "        \"\"\"\n",
    "        quantizing 16 bit audio inputs to 0-255 range using meu law quantization\n",
    "        assuming the audio input is already normalized in range [-1, 1]\n",
    "        \"\"\"\n",
    "        x = np.array(x)\n",
    "        u = 255\n",
    "        mu_law_output = np.sign(x) * (np.log(1 + u*np.abs(x)))/(np.log(1+u))\n",
    "        quantized = ((mu_law_output + 1)/2 * 255).astype(np.uint8)\n",
    "        return torch.tensor(quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"C:\\Users\\tanbi\\door\\hall_projects\\audio_replicate\\models\\waveNET\\Datasets\"\n",
    "out_path = r\"C:\\Users\\tanbi\\door\\hall_projects\\audio_replicate\\models\\waveNET\\data\\processed_data2\"\n",
    "dataset = WaveNetDataset(dataset_path, out_path, input_length=16000, target_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2832018"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([127., 127., 127.,  ..., 127., 127., 126.]), tensor([127.]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target = dataset[0]\n",
    "input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_testing = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([148., 142., 143.,  ..., 108., 113., 129.]), tensor([115.]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data valid uptil index \"-16002\", have to fix it and make it valid for all cases but hopefull should work fine by hard coding and ignoring this case\n",
    "dataloader_testing.dataset[-16002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-4.6272e-06, -2.7477e-06,  4.4635e-06,  ...,  1.8226e-05,\n",
      "         3.0116e-05,  1.9175e-06]), tensor([-7.7950e-05]))\n"
     ]
    }
   ],
   "source": [
    "print(dataset[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
